# --------------------
# Global Flags
# --------------------
-‒location : Where to run following command.

-‒format :  Specifies the format of the command's output.
    - pretty : formatted table output
    - sparse : simpler table output
    - prettyjson : easy-to-read json format
    - json : compact JSON
    - csv : CSV format with header

--job_id : Explicitly give name to a job. Used with those commands that create a job.
    Example: cp, load, extract, etc.


# --------------------
# Query command Flags
# --------------------
--append_table or -a: Boolean flag. When specified, query results are append to a table.
--destination_table : Table where to write the query results.
--replace : Flag to overwrite the destination table with query results.
--destination_schema : If the destination table is not present, it will get created with specified squema
--time_partitioning_field : If you want to partition the destition table on some time. Provide column
--time_partitioning_type : Ingestion time partition table. Example --time_partitioning_type=DAY
--time_partitioning_expiration : Set the partition expiration time in seconds.
--clustering_fields : Specify upto 4 comma-separated columns on which you want to cluster the destination table.
--destination_kms_key : Provide the resource id of customer generated encryption key
--batch : Keep it to true if you want the query to run in batch mode.
--maximum_bytes_billed : Flag to limit the bytes billed for query.
--label : Flag to apply labels to a query job in the form of key value pairs.
--dry_run : Validator flag to check the correctness of a query and bytes to be processed.
--max_rows : Integer specifying the number of rows to return in the query results.
--require-cache : If specified, query will only run if the results can be retrieved from the cache otherwise not.
--use_cache : Set it to false if you do not want the current query to use cached results.
--nouse_cache : Equivalent to --use_cache=false
--schedule : Makes a recurring scheduled query. Example: --schedule = 'every 24 hours'
--display_name : Name give to the schedule
--target_dataset : Write scheduled query results into a destination table.
--allow_large_results : Enables large destination table sizes for legcy SQL queries.
--flatten_results : Boolean field to get flattened results of nested and repeated fields.
--udf_resource : This flag will contain the Cloud storage Uri or the pat to a local UDF code file.
--range_partitioning [col-name], [lower-bound], [upper-bound], [interval]


# ----------------------------
# BQ CLI EXCLUSIVE OPERATIONS 
# ----------------------------

# Relaxing a column mode to nullable
--schema_update_option : It has 2 values ALLOW_FIELD_ADDITION and ALLOW_FIELD_RELAXATION

# --------------------
# CP command Flags
# --------------------

--append_table or -a: Boolean flag. When specified, query results are append to a table.
--destination_kms_key : Provide the resource id of customer generated encryption key
--force or f : when specified, if the destination table exists, overwrite it. Default value is false
--no_clobber or -n : when specified, if the destination table exists, do not overwrite it. Default value is false


# ----------------
# Show command
# ----------------

# Show Dataset 
bq show --dataset bigquery-demo-354214:dataset1

# Show schema
bq show --schema dataset1.names
bq show --schema --format prettyjson dataset1.names

# List entities inside a collection
bq ls
bq ls [dataset-name]

# ----------------
# Help command
# ----------------
bq help 

# Canel (you can provide the name of a job and it will be canceled)
bq cancel [job-name]

# Interactive mode (Shell)
# To exit type exit
bq shell
# ----------------
# Query command
# ----------------
bq query --use_legacy_sql=false 'SELECT * FROM `bigquery-demo-354214`.dataset1.names'

bq query \
--use_legacy_sql=false \
--append_table=false \
--destination_table \
--clustering_fields \
--batch=false \
--maximum_bytes_billed=30000000 \
--label key1:value1 \
--label key2:value2 \
--dry_run \
'SELECT * FROM `bigquery-demo-354214`.dataset1.names limit 10'

bq query \
--use_legacy_sql=false \
--label dummy_key1:value1 \
--label dummy_key2:value2 \
--batch=false \
--maximum_bytes_billed=30000000 \
--require_cache=false \
--destination_table=bigquery-demo-354214:dataset1.names_ggg \
--destination_schema names:string,gend:string,count:integer \
--time_partitioning_field \
--clustering_fields=name \
--time_partitioning_expiration=90000 \
SELECT * FROM `bigquery-demo-354214.dataset1.names` limit 10

# ----------------
# Create dataset
# ----------------
bq mk --dataset --default_table_expiration 4000 --default_partition_expiration 5000 --description "New dataset" bigquery-demo-354214:dataset_bq

# ------------------
# Create table
# ------------------
bq mk \
--table \
--expiration 3000 \
--description "New table from BQ" \
--label du1:v1 \
--label du2:v2 \
--require_partition_filter=false \
--time_partitioning_type DAY \
--time_partitioning_expiration 6000 \
--clustering_fields name \
--schema name:string,gender:string,count:integer dataset1.names_from_bq

# --------------------------------------
# Create table with json file as schema
# --------------------------------------
bq mk \
--table \
--expiration 3000 \
--description "New table from BQ" \
--label du1:v1 \
--label du2:v2 \
--require_partition_filter=false \
--time_partitioning_type DAY \
--time_partitioning_expiration 6000 \
--clustering_fields name \
--schema /home/alonsmd/Downloads/BQ_json_schema.txt dataset1.names_from_bq_json

# -------------------------------------------------------------------------------
# Load commands 
# Reference: https://cloud.google.com/bigquery/docs/reference/bq-cli-reference
# -------------------------------------------------------------------------------
bq load \
bigquery-demo-354214:dataset1.names_from_cli \
/home/alonsmd/Downloads/names/yob1880.txt /home/alonsmd/Downloads/BQ_json_schema.txt 

bq query --use_legacy_sql=false 'select * from `bigquery-demo-354214`.dataset1.names_from_cli limit 10'



# -------------------------------------------------------------------------------
# Partition Operations 
# -------------------------------------------------------------------------------
# Load data and relaxing column
bq load \
--schema_update_option ALLOW_FIELD_RELAXATION \
bigquery-demo-354214:dataset1.tab_req /home/alonsmd/Downloads/names/yob1880.txt name:string,gender:string,count:integer

# Copy a specific partition to another table
bq cp -a 'dataset1.demo_part_date$20200117' dataset1.dest_table
bq cp -a 'dataset1.demo_part_date$20200119' dataset1.dest_table
bq cp -a dataset1.demo_part_date$20200119,dataset1.demo_part_date$20200118 dataset1.dest_table

select * from [dataset1.dest_table$__PARTITIONS_SUMMARY__]


bq rm dataset1.part_ing_2$2020112005

# Copy a partition data into another partition
bq cp -f 'dataset1.part_ing_1$20220626' 'dataset1.part_ing_2$20220627'

select count(*) from `bigquery-demo-354214.dataset1.part_ing_2` where _PARTITIONTIME = TIMESTAMP("2020-11-20 10:00:00")
select count(1) as q from `bigquery-demo-354214.dataset1.part_ing_2` where _PARTITIONDATE = DATE("2022-06-27")